{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.842591Z",
     "start_time": "2021-11-12T18:12:37.590Z"
    }
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.847577Z",
     "start_time": "2021-11-12T18:12:37.592Z"
    }
   },
   "outputs": [],
   "source": [
    "# load Data\n",
    "df = pd.read_csv(\"C:/Users/whgod/Desktop/머신러닝/Lab2/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.849592Z",
     "start_time": "2021-11-12T18:12:37.595Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe() \n",
    "# we need to check the total_bedrooms count. (Less than others.)\n",
    "# we need to scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.851593Z",
     "start_time": "2021-11-12T18:12:37.597Z"
    }
   },
   "outputs": [],
   "source": [
    "# As the population grows, total_bedrooms is also expected to increase.\n",
    "# After sorting the data, check the correlation.\n",
    "df_sort = df.sort_values(by=['population']) ; df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.853593Z",
     "start_time": "2021-11-12T18:12:37.600Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(df_sort['population'], df_sort['total_bedrooms'])\n",
    "# We can visually see that the values of the two variables are positively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.855578Z",
     "start_time": "2021-11-12T18:12:37.602Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can handle missing values using the ffill method.\n",
    "df_clean = df_sort.fillna(method='ffill')\n",
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.857595Z",
     "start_time": "2021-11-12T18:12:37.605Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the clean data and reset index.\n",
    "DF = df_clean.reset_index(drop='index'); DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.860577Z",
     "start_time": "2021-11-12T18:12:37.607Z"
    }
   },
   "outputs": [],
   "source": [
    "def AutoScaleEncode(DataFrame, scalers, encoder, encoding_col) :\n",
    "    \n",
    "    \"\"\"\n",
    "    It receives the desired data frame and scale function as variables and returns a data frame.\n",
    "    --------------------------------\n",
    "    DataFrame : DataFrame\n",
    "    scalers : The scaler you want to apply\n",
    "    encoder : The encoder you want to apply\n",
    "    encoding_col : The columns you want to encode\n",
    "    \"\"\"\n",
    "    Df_scale = DataFrame.drop([encoding_col], axis=1)\n",
    "    Df_encode = DataFrame[[encoding_col]]\n",
    "    \n",
    "    scaler = scalers()\n",
    "    scaled = scaler.fit_transform(Df_scale)\n",
    "    scaled = pd.DataFrame(scaled, columns= Df_scale.columns)\n",
    "    \n",
    "    if encoder == OrdinalEncoder():\n",
    "        enc = encoder\n",
    "        enc = enc.fit_transform(Df_encode)\n",
    "        enc = pd.DataFrame(enc, columns= Df_encode.columns)\n",
    "        \n",
    "        new_df = pd.concat([scaled, enc], axis=1)\n",
    "    else:\n",
    "        dum = pd.get_dummies(Df_encode)\n",
    "        dum = pd.DataFrame(dum)\n",
    "        \n",
    "        new_df = pd.concat([scaled, dum], axis=1)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.863576Z",
     "start_time": "2021-11-12T18:12:37.610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use AutoScaler\n",
    "candidate = AutoScaleEncode(DF, MinMaxScaler, OneHotEncoder, 'ocean_proximity'); candidate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.866600Z",
     "start_time": "2021-11-12T18:12:37.613Z"
    }
   },
   "outputs": [],
   "source": [
    "# hitmap plot\n",
    "corr = candidate.corr();corr\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(data = corr,\n",
    "            annot = True,\n",
    "            mask = mask,\n",
    "            fmt = '.2f',\n",
    "            linewidths = 1.,\n",
    "            cmap = 'RdYlBu_r')\n",
    "plt.show()\n",
    "\n",
    "# We can see that the latitude and longitude variables are highly correlated.\n",
    "# We can also see that the three variables are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.868579Z",
     "start_time": "2021-11-12T18:12:37.615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X= candidate[['total_bedrooms', 'total_rooms']] ; X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.870598Z",
     "start_time": "2021-11-12T18:12:37.619Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "printcipalComponents = pca.fit_transform(candidate)\n",
    "principalDf = pd.DataFrame(data=printcipalComponents, columns = ['principal component1', 'principal component2'])\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.873585Z",
     "start_time": "2021-11-12T18:12:37.622Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.875582Z",
     "start_time": "2021-11-12T18:12:37.624Z"
    }
   },
   "outputs": [],
   "source": [
    "candidate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.880602Z",
     "start_time": "2021-11-12T18:12:37.628Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "    tmpdf = X\n",
    "\n",
    "    ClusterParams = []\n",
    "    kmeansParams = {\n",
    "        'n_clusters' : [2, 4, 6, 8, 10, 12],\n",
    "        'algorithm' : ['auto', 'full', 'elkan'],\n",
    "        'init' : ['k-means++', 'random']\n",
    "    }\n",
    "    \n",
    "    dbscanParams = {\n",
    "        'eps' : [0.5, 1.0],\n",
    "        'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size' : [3, 5, 15, 20],\n",
    "        'min_samples' : [3, 5, 10, 15]\n",
    "    }\n",
    "\n",
    "    GMParams = {\n",
    "        'n_components' : [2, 4, 6, 8, 10, 12],\n",
    "        'convariance_type' : ['full', 'tied', 'diag', 'spherical'],\n",
    "        'init_params' : ['kmeans', 'random']\n",
    "    }\n",
    "\n",
    "    claransParams = {\n",
    "        'number_clusters' : [2, 4, 6, 8, 10, 12],\n",
    "        'numlocal' : [2, 4, 8, 10],\n",
    "        'maxneighbor' : [3, 5, 15]\n",
    "    }\n",
    "\n",
    "    MSParams = {\n",
    "        'bandwidth' : [0.7, 1.3, 2.576979121414909, 5]\n",
    "    }\n",
    "\n",
    "    ClusterParams.append(kmeansParams)\n",
    "    ClusterParams.append(dbscanParams)\n",
    "    ClusterParams.append(GMParams)\n",
    "    ClusterParams.append(claransParams)\n",
    "    ClusterParams.append(MSParams)\n",
    "\n",
    "    scores, labels, bestScore, bestResult, bestIndex = findBestParams(tmpdf,ClusterParams)\n",
    "\n",
    "    print(\"best params = \" + str(labels[bestIndex]))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.885582Z",
     "start_time": "2021-11-12T18:12:37.631Z"
    }
   },
   "outputs": [],
   "source": [
    "principalDf = principalDf.drop(['MHV_cut'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.887584Z",
     "start_time": "2021-11-12T18:12:37.632Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.qcut(candidate.median_house_value,[0, 0.375, 0.625, 1], labels=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.878599Z",
     "start_time": "2021-11-12T18:12:37.626Z"
    }
   },
   "outputs": [],
   "source": [
    "def findBestParams(data, ClusterParams):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compare the performance (silhouet score) of different clustering models on the same data set.\n",
    "    return scores, labels, bestScore, bestResult, bestIndex.\n",
    "    ------------------------------------------------------------------------------\n",
    "    data : DataFrame \n",
    "    ClusterParams : the model you want to compare\n",
    "        None :  kmeansParams = {\n",
    "                    'n_clusters' : [2, 4, 6, 8, 10, 12],\n",
    "                    'algorithm' : ['auto', 'full', 'elkan'],\n",
    "                    'init' : ['k-means++', 'random']\n",
    "                }\n",
    "    \n",
    "                dbscanParams = {\n",
    "                    'eps' : [0.5, 1.0],\n",
    "                    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                    'leaf_size' : [3, 5, 15, 20],\n",
    "                    'min_samples' : [3, 5, 10, 15]\n",
    "                }\n",
    "\n",
    "                GMParams = {\n",
    "                        'n_components' : [2, 4, 6, 8, 10, 12],\n",
    "                        'convariance_type' : ['full', 'tied', 'diag', 'spherical'],\n",
    "                        'init_params' : ['kmeans', 'random']\n",
    "                }\n",
    "\n",
    "                claransParams = {\n",
    "                        'number_clusters' : [2, 4, 6, 8, 10, 12],\n",
    "                        'numlocal' : [2, 4, 8, 10],\n",
    "                        'maxneighbor' : [3, 5, 15]\n",
    "                }\n",
    "\n",
    "                MSParams = {\n",
    "                        'bandwidth' : [0.7, 1.3, 2.576979121414909, 5]\n",
    "                }\n",
    "    \"\"\"\n",
    "\n",
    "    threads = 16\n",
    "\n",
    "    scores = []\n",
    "    labels = []\n",
    "    bestScore = -100.0\n",
    "    bestResult = 0\n",
    "    bestIndex = 0\n",
    "\n",
    "    for n_clusters in ClusterParams[0]['n_clusters']:\n",
    "        for algorithm in ClusterParams[0]['algorithm']:\n",
    "            for init in ClusterParams[0]['init']:\n",
    "\n",
    "                paramLabel = \", method : KMeans, n_cluster : \" + str(n_clusters) + \", algorithm : \" + str(algorithm) + \", init : \" + str(init)\n",
    "                        \n",
    "                km = KMeans(n_clusters=n_clusters, algorithm=algorithm, init=init)\n",
    "                result = km.fit_predict(data)\n",
    "                try:\n",
    "                    score = silhouette_score(data, result)\n",
    "                except ValueError as e:\n",
    "                    score = -1\n",
    "                    pass\n",
    "\n",
    "                label = paramLabel + \", score :\" + str(score)\n",
    "                print(label)\n",
    "\n",
    "                scores.append(score)\n",
    "                labels.append(label)\n",
    "\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestResult = result\n",
    "                    bestIndex = len(scores) - 1\n",
    "\n",
    "\n",
    "    for eps in ClusterParams[1]['eps']:\n",
    "        for algorithm in ClusterParams[1]['algorithm']:\n",
    "            for leaf_size in ClusterParams[1]['leaf_size']:\n",
    "                for min_samples in ClusterParams[1]['min_samples']:\n",
    "\n",
    "                    paramLabel = \", method : DBSCAN , eps : \" + str(eps) + \", algorithm : \" + str(algorithm) + \", leaf_Size : \" + str(leaf_size) + \", min_samples : \" + str(min_samples)\n",
    "\n",
    "                    db = DBSCAN(eps=eps, algorithm=algorithm, leaf_size=leaf_size, min_samples=min_samples)\n",
    "                    result = db.fit_predict(data)\n",
    "                    try:\n",
    "                        score = silhouette_score(data, result)\n",
    "                    except ValueError as e:\n",
    "                        score = -1\n",
    "                        pass\n",
    "                    label = paramLabel + \", score :\" + str(score)\n",
    "                    print(label)\n",
    "\n",
    "                    scores.append(score)\n",
    "                    labels.append(label)\n",
    "\n",
    "                    if score > bestScore:\n",
    "                        bestScore = score\n",
    "                        bestResult = result\n",
    "                        bestIndex = len(scores) - 1\n",
    "            \n",
    "            \n",
    "    for n_components in ClusterParams[2]['n_components']:\n",
    "        for convariance_type in ClusterParams[2]['convariance_type']:\n",
    "            for init_params in ClusterParams[2]['init_params']:\n",
    "\n",
    "                paramLabel = \", method : GaussianMixture, n_components : \" + str(n_components) + \", convariance_type : \" + str(convariance_type) + \", init_params : \" + str(init_params)\n",
    "\n",
    "                gm = GaussianMixture(n_components=n_components, covariance_type=convariance_type, init_params=init_params)\n",
    "                result = gm.fit_predict(data)\n",
    "                try:\n",
    "                    score = silhouette_score(data, result)\n",
    "                except ValueError as e:\n",
    "                    score = -1\n",
    "                    pass\n",
    "                label = paramLabel + \", score :\" + str(score)\n",
    "                print(label)\n",
    "\n",
    "                scores.append(score)\n",
    "                labels.append(label)\n",
    "\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestResult = result\n",
    "                    bestIndex = len(scores) - 1\n",
    "                    \n",
    "    for number_clusters in ClusterParams[3]['number_clusters']:\n",
    "        for numlocal in ClusterParams[3]['numlocal']:\n",
    "            for maxneighbor in ClusterParams[3]['maxneighbor']:\n",
    "\n",
    "                paramLabel = \", method : CLARANS, number_clusters : \" + str(number_clusters) + \", numlocal : \" + str(numlocal) + \", maxneighbor : \" + str(maxneighbor)\n",
    "\n",
    "                cl = clarans(np.array(data), number_clusters, numlocal, maxneighbor)\n",
    "                result = cl.process()\n",
    "                score = silhouette_score(data, result.get_cluster_encoding)\n",
    "\n",
    "                label = paramLabel + \", score :\" + str(score)\n",
    "                print(label)\n",
    "\n",
    "                scores.append(score)\n",
    "                labels.append(label)\n",
    "\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestResult = result\n",
    "                    bestIndex = len(scores) - 1  \n",
    "                            \n",
    "    for bandwidth in ClusterParams[3]['bandwidth']:\n",
    "                \n",
    "        paramLabel = \", method : MeanShift, bandwidth : \" + str(bandwidth)\n",
    "\n",
    "        ms = MeanShift(bandwidth=bandwidth, n_jobs=threads)\n",
    "        result = ms.fit_predict(data)\n",
    "        try:\n",
    "            score = silhouette_score(data, result)\n",
    "        except ValueError as e:\n",
    "            score = -1\n",
    "            pass\n",
    "        label = paramLabel + \", score :\" + str(score)\n",
    "        print(label)\n",
    "\n",
    "        scores.append(score)\n",
    "        labels.append(label)\n",
    "\n",
    "        if score > bestScore:\n",
    "            bestScore = score\n",
    "            bestResult = result\n",
    "            bestIndex = len(scores) - 1\n",
    "            \n",
    "    return scores, labels, bestScore, bestResult, bestIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T18:12:37.883589Z",
     "start_time": "2021-11-12T18:12:37.629Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['latitude', 'total_rooms', 'median_house_value']]\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "scaled = pd.DataFrame(scaled, columns=X.columns)\n",
    "label = GaussianMixture(n_components=2,covariance_type='tied',init_params='kmeans').fit_predict(X)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(X['total_bedrooms'],X['total_rooms'], c=label)\n",
    "plt.xlabel('total_bedrooms')\n",
    "plt.ylabel('total_rooms')\n",
    "plt.title('Gaussian Mixture clustering')\n",
    "\n",
    "X['MHV_cut'] = pd.qcut(candidate.median_house_value,[0, 0.5, 1], labels=[0,1])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(X['total_bedrooms'],X['total_rooms'], c= X['MHV_cut'])\n",
    "plt.xlabel('total_bedrooms')\n",
    "plt.ylabel('total_rooms')\n",
    "plt.title('Mean_house_value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
